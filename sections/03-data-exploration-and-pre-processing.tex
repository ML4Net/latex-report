% 03-data-exploration-and-pre-processing.tex

% Data Exploration and Pre-processing
% 3.1. Introduction: Introduces the data exploration and pre-processing tasks.
% 3.2. Dataset Preparation: Describes the process of loading the dataset and initial inspection.
% 3.3. Temporal Analysis: Analyzes when the attacks were performed.
% 3.4. Feature Extraction: Extracts features from the attack sessions.
% 3.5. Common Words Analysis: Identifies the most common words in the sessions.
% 3.6. Intent Distribution: Analyzes the distribution of intents over time.
% 3.7. Text Representation: Converts text into numerical representations (BoW, TF-IDF).

% Section Title
\section{DATA EXPLORATION AND PRE-PROCESSING}

    % Main Content

    \subsection{Introduction}

        This section details the data exploration and pre-processing steps undertaken to analyze SSH shell attack logs. The tasks include dataset preparation, temporal analysis, feature extraction, common words analysis, intent distribution, and text representation.

    \subsection{Dataset Preparation}
    
        The dataset used in this research is loaded from a Parquet file (\texttt{ssh\_attacks.parquet}) into a Pandas DataFrame. The initial inspection involves checking the dataset's structure, identifying missing values, and detecting duplicate rows. The dataset contains columns such as \texttt{Session ID}, \texttt{Full session text}, \texttt{Timestamp}, and \texttt{Intent labels}.

        \begin{verbatim}
            # Load the dataset
            SSH_Attacks = pd.read_parquet("../data/processed/ssh_attacks_decoded.parquet")
    
            # Inspect the dataset structure
            print(SSH_Attacks.info())
    
            # Check for missing values
            print(SSH_Attacks.isnull().sum())
    
            # Check for duplicate rows
            print(SSH_Attacks.duplicated().sum())
        \end{verbatim}

        The initial inspection revealed that the dataset is well-structured with columns that are essential for our analysis. However, it is important to handle any missing values and duplicates to ensure the integrity of the data. The following steps were taken to address these issues:

        \begin{itemize}
            \item **Missing Values**: We identified and handled missing values by either imputing them with appropriate values or removing the affected rows.
            \item **Duplicate Rows**: Duplicate rows were detected and removed to avoid redundancy in the analysis.
        \end{itemize}

        \textbf{Placeholder for Dataset Structure Table}

    \subsection{Temporal Analysis}
    
        The temporal analysis examines when the attacks were performed. The \texttt{first\_timestamp} column is converted to a datetime format to analyze attack frequencies over time, including hourly, daily, and monthly trends.

        \begin{verbatim}
            # Convert first_timestamp to datetime format
            SSH_Attacks['first_timestamp'] = pd.to_datetime(SSH_Attacks['first_timestamp'])

            # Analyze attack frequencies over time
            temporal_series = (
                SSH_Attacks.groupby(SSH_Attacks['first_timestamp'].dt.date)
                .size()
                .reset_index(name='attack_count')
            )
        \end{verbatim}

        The analysis includes plotting the number of attacks per hour, month, and year to identify patterns and trends. This helps in understanding the temporal distribution of attacks and identifying any periodic patterns or anomalies.

        \textbf{Placeholder for Temporal Analysis Plot}

        The temporal analysis revealed that the frequency of attacks varies significantly over time. By examining the hourly, daily, and monthly trends, we can gain insights into the behavior of attackers and the times when systems are most vulnerable.

    \subsection{Feature Extraction}
    
        Feature extraction involves identifying and extracting relevant features from the attack sessions. This includes analyzing the distribution of classes (intents) and visualizing the data using bar plots.

        \begin{verbatim}
            # Extract and count occurrences of each class
            all_classes = SSH_Attacks['Set_Fingerprint'].explode().str.strip()
            class_counts = all_classes.value_counts()

            # Plot the distribution of classes
            sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')
        \end{verbatim}

        The distribution of classes provides valuable information about the types of attacks and their prevalence. By visualizing this distribution, we can identify the most common attack intents and focus our analysis on these areas.

        \textbf{Placeholder for Class Distribution Bar Plot}

        The feature extraction process also involves creating new features that can enhance the analysis. For example, we can extract the length of each session, the number of commands executed, and other relevant metrics.

    \subsection{Common Words Analysis}
    
        The common words analysis identifies the most frequent words used in the attack sessions. This is achieved using word clouds and other text analysis techniques.

        \begin{verbatim}
            # Generate a word cloud for the session text
            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(SSH_Attacks['Full session text']))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.show()
        \end{verbatim}

        The word cloud visualization highlights the most common words used in the attack sessions, providing insights into the attackers' behavior and strategies. This can help in identifying common commands and patterns used in the attacks.

        \textbf{Placeholder for Word Cloud Visualization}

        Additionally, we can create bar plots to show the frequency of the top 10 most common words, which can further aid in understanding the textual characteristics of the attack sessions.

        \textbf{Placeholder for Common Words Bar Plot}

    \subsection{Intent Distribution}
            
        The intent distribution analysis examines the distribution of intents over time. This involves grouping the data by date and intent to count occurrences and visualize the trends.

        \begin{verbatim}
            # Group by Set_Fingerprint and date to count occurrences
            grouped_SSH_Attacks = (
                SSH_Attacks.explode('Set_Fingerprint')
                .groupby([SSH_Attacks['first_timestamp'].dt.date, 'Set_Fingerprint'])
                .size()
                .reset_index(name='attack_count')
            )
        \end{verbatim}

        By analyzing the distribution of intents over time, we can identify trends and patterns in the attackers' behavior. This can help in understanding how different types of attacks evolve and vary over time.

        \textbf{Placeholder for Intent Distribution Plot}

        The intent distribution analysis also helps in identifying any seasonal or periodic patterns in the attacks, which can be crucial for developing effective defense strategies.

    \subsection{Text Representation}
    
        Text representation converts the session text into numerical representations using techniques such as Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF). These representations are used for further analysis and machine learning tasks.

        \begin{verbatim}
            # Convert text into numerical representations using Bag of Words (BoW)
            from sklearn.feature_extraction.text import CountVectorizer
            bow_vectorizer = CountVectorizer()
            X_bow = bow_vectorizer.fit_transform(SSH_Attacks['Full session text'])

            # Convert text into numerical representations using TF-IDF
            from sklearn.feature_extraction.text import TfidfVectorizer
            tfidf_vectorizer = TfidfVectorizer()
            X_tfidf = tfidf_vectorizer.fit_transform(SSH_Attacks['Full session text'])
        \end{verbatim}

        The resulting numerical representations from both BoW and TF-IDF are normalized and used for subsequent analysis and modeling. These representations are essential for applying machine learning algorithms to classify and predict attack intents.

        \textbf{Placeholder for Text Representation Table}

        The text representation techniques help in transforming the unstructured session text into structured numerical data, which can be used for various analytical and predictive tasks. By comparing the performance of different representation techniques, we can select the most effective method for our analysis.
