% 05-unsupervised-learning-clustering.tex

% Unsupervised Learning – Clustering
% 5.1. Introduction: Provides an overview of the clustering task and its objectives.
% 5.2. Determine the Number of Clusters: Uses methods like the elbow method or silhouette analysis to determine the number of clusters.
% 5.3. Hyperparameter Tuning: Tunes other hyperparameters, if any.
% 5.4. Cluster Visualization: Visualizes the clusters through t-SNE.
% 5.5. Cluster Analysis: Analyzes the characteristics of each cluster.
% 5.6. Intent Homogeneity: Assesses if clusters reflect intent division.
% 5.7. Specific Attack Categories: Associates clusters with specific attack categories.

% Section Title
\section{UNSUPERVISED LEARNING - CLUSTERING}

    % Main Content

    \subsection{Introduction}
    
        Unsupervised learning, a powerful branch of machine learning, was applied in this project to gain insights from SSH attack data. The primary focus was on leveraging clustering methods to group similar attack sessions based on their intrinsic patterns and characteristics. By analyzing these groups, the study aimed to uncover hidden relationships and categorize different attack intents and behaviors without relying on predefined labels. 
    
    \subsection{Data Preparation}
    
        The dataset chosen was the one generated through the TF-IDF vectorization technique. This was made because it was essential to start with a dataset that represented in the best way the frequency and the importance of words, making each word as a dimension of our vector.

    \subsection{Clustering Methods}
    
        Clustering techniques were employed to uncover natural groupings within the dataset, providing insights into SSH attack patterns. The following methods were used:
        
        \begin{itemize}
        
            \item \textbf{K-Means Clustering}: The algorithm iteratively assigns each data point to the nearest cluster centroid and updates the centroids until convergence. The Elbow Method was applied to determine the optimal number of clusters by examining the total within-cluster sum of squares (inertia). Silhouette scores were also calculated to evaluate the cohesion and separation of clusters, ensuring the clustering results were meaningful and well-separated.
            
            \item \textbf{Gaussian Mixture Model (GMM)}: Unlike K-Means, GMM considers the probability of each data point belonging to a cluster, providing a more flexible and nuanced clustering approach. The optimal number of clusters was determined using a combination of log-likelihood scores, which measure how well the model fits the data, and silhouette analysis to validate cluster quality. This dual approach ensured the GMM provided reliable and interpretable clustering results.
            
        \end{itemize}
        
    \subsection{Clustering Evaluation Techniques}
    
        \subsubsection{K-Means Clustering \\}
            
            The evaluation of the K-Means clustering results is based on the Elbow Method and the Silhouette Score, which provide insights into the optimal number of clusters for the dataset. The Elbow Method graph shows a steep decline in clustering error between 3 and 6 clusters, followed by a more gradual decrease as the number of clusters increases. The point of inflection, or "elbow," appears around 6 clusters, suggesting that adding more clusters beyond this point results in diminishing improvements in minimizing intra-cluster variance. The Silhouette Score graph exhibits a rapid increase up to 5 clusters, reaching a stable high value of approximately 0.95. A drop is observed around 8 clusters, after which the score gradually increases again, peaking beyond 12 clusters. This pattern indicates that a smaller number of clusters (around 5-6) achieves a strong balance of cohesion and separation, while additional clusters beyond 12 continue to refine the structure with marginal improvements. Considering both metrics, the optimal number of clusters for K-Means is likely between 5 and 6, ensuring a trade-off between clustering accuracy and computational efficiency.
        
        \subsubsection{Gaussian Mixture Model (GMM) \\}

            The evaluation of the GMM clustering results is based on the Silhouette Score and the Log-Likelihood Score, both of which provide insights into the quality of the clustering structure.

            The Silhouette Score graph shows a sharp increase up to 5 clusters, reaching a stable high value around 0.95. A slight drop is observed at 8 clusters, followed by a steady increase, with the highest scores occurring beyond 12 clusters. This suggests that increasing the number of clusters generally improves separation and cohesion, though the optimal balance appears to be around 6 clusters, where the highest stable performance is first achieved.

            The Log-Likelihood Score graph indicates a rapid increase from 3 to 5 clusters, after which the improvements become more gradual. Beyond 12 clusters, the score stabilizes, indicating diminishing returns in model fitting. This suggests that while increasing the number of clusters provides better representation of the data, the most significant improvements occur within the first few increments.

            Considering both metrics, an optimal cluster configuration is likely between 6 and 8 clusters, balancing cluster separation, model likelihood, and computational efficiency..

        % \vspace{-0.3cm}
        
        \begin{figure}[h]
            \centering
            \begin{minipage}[c]{0.47\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../figures/plots/section3/k-means_clustering_error.png}
                \caption{K-means Elbow Method}
                \label{fig:tsne_kmeans}
            \end{minipage}
            \hfill
            \begin{minipage}[c]{0.47\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../figures/plots/section3/k-means_silohuette_score.png}
                \caption{K-means Silhouette Score}
                \label{fig:tsne_gmm}
            \end{minipage}
        \end{figure}
        
        % \vspace{-0.5cm}
        
        \begin{figure}[h]
            \centering
            \begin{minipage}[c]{0.47\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../figures/plots/section3/gmm_total_log-likelihood_score.png}
                \caption{GMM Log-Likelihood Score}
                \label{fig:tsne_kmeans}
            \end{minipage}
            \hfill
            \begin{minipage}[c]{0.47\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../figures/plots/section3/gmm_silohuette_score.png}
                \caption{GMM Silhouette Score}
                \label{fig:tsne_gmm}
            \end{minipage}
        \end{figure}

    \subsection{Hyperparameter Tuning}
    
    To optimize both the K-Means and Gaussian Mixture Model (GMM) clustering approaches, hyperparameter tuning was performed using grid search with cross-validation. For K-Means clustering, the optimal parameters identified were:
    \begin{itemize}
        \item \textbf{Initialization method}: k-means++
        \item \textbf{Number of initializations (n\_init)}: 10
        \item \textbf{Maximum iterations (max\_iter)}: 50
    \end{itemize}
    The final clustering results achieved a silhouette score of 0.9490 and an inertia value of 1708.96. The high silhouette score indicates well-defined and well-separated clusters, while the minimized inertia suggests an efficient clustering structure. For the Gaussian Mixture Model (GMM), the best hyperparameters found were:
    \begin{itemize}
        \item \textbf{Covariance Type}: full
        \item \textbf{Initialization method}: k-means
        \item \textbf{Maximum iterations (max\_iter)}: 50
        \item \textbf{Tolerance (tol)}: 0.001
    \end{itemize}
    The model's performance was evaluated using a silhouette score of 0.9398 and a log-likelihood score of 508.71. The silhouette score confirms strong cluster separation, while the high log-likelihood indicates that the GMM model effectively captures the underlying distribution of the data.
    
    Both clustering techniques provided effective segmentation of the dataset. K-Means exhibited a slightly higher silhouette score, suggesting better-defined cluster boundaries, while GMM’s higher log-likelihood highlights its ability to model complex, overlapping clusters.
    \subsection{Clusters Visualization}
    
        To better understand the structure of the clusters, t-SNE dimensionality reduction was applied. While t-SNE is effective for visualizing high-dimensional data, its interpretation must align with clustering validation metrics. 
        
        \begin{figure}[h]
            \centering
            \begin{minipage}[c]{0.47\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../figures/plots/section3/tsne_kmeans_clusters.png}
                \caption{t-SNE Visualization of K-Means Clusters}
                \label{fig:tsne_kmeans}
            \end{minipage}
            \hfill
            \begin{minipage}[c]{0.47\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../figures/plots/section3/tsne_gmm_clusters.png}
                \caption{t-SNE Visualization of GMM Clusters}
                \label{fig:tsne_gmm}
            \end{minipage}
        \end{figure}

        \subsubsection{K-Means Visualization \\}

        The t-SNE visualization of K-Means clustering reveals that a significant portion of the dataset is assigned to a dominant red cluster. While this cluster appears dense, it raises concerns about the method’s ability to differentiate between distinct attack types. The blue and purple clusters are more compact and well-separated, suggesting a few meaningful subgroups. However, several smaller clusters are sparsely distributed, potentially indicating outliers rather than well-formed attack patterns. This visualization suggests that a majority of attacks are grouped together, limiting the effectiveness of this approach.

        \subsubsection{GMM Visualization \\}
        
        The t-SNE visualization of GMM clustering reveals a different structure. Unlike K-Means, which forces strict cluster assignments, GMM probabilistically associates data points with multiple clusters. The resulting visualization displays smoother transitions between clusters, particularly in peripheral areas. However, a large central cluster remains dominant, similar to K-Means. This suggests that while GMM captures more complex attack distributions, it does not necessarily provide significantly better differentiation between attacks. The presence of overlapping points implies that GMM is better suited for identifying gradual variations in attack behaviors rather than strict categorization.

    \subsection{Clusters Analysis}

        \subsubsection{Word Cloud Representation \\}

            Word clouds were generated to highlight dominant terms in each cluster. The clusters display considerable overlap in command usage.

            \begin{itemize}
                \item Clusters 1, 7, and 9 prominently feature terms such as \texttt{grep}, \texttt{var}, and \texttt{tmp}, suggesting an emphasis on system navigation and information retrieval. However, similar terms appear in multiple clusters, making it difficult to distinguish attack types based solely on these keywords.
                \item Clusters 0 and 3 highlight \texttt{unix}, \texttt{tmp}, and \texttt{x19}, which may indicate system configuration activity. However, these terms do not clearly define unique attack patterns.
                \item Clusters 5 and 8 contain terms like \texttt{chmod}, \texttt{wget}, and \texttt{rm}, suggesting file modification and deletion activities. While these behaviors align with attack patterns, their occurrence in multiple clusters weakens their ability to define distinct groups.
                \item Cluster 4 prominently features \texttt{ssh}, \texttt{authorizedkeys}, and \texttt{passwd}, aligning well with authentication-related attacks.
                \item Cluster 2, associated with \texttt{busybox} and \texttt{mounts}, appears indicative of IoT-related attack scenarios, a distinction that adds value to the clustering results.
                \item Cluster 6 is characterized by \texttt{shell}, \texttt{system}, and \texttt{name}, but these are general system commands that appear in multiple clusters, reducing their discriminative power.
            \end{itemize}
            
            While word clouds help in summarizing cluster features, the significant overlap in dominant terms suggests that the clustering results may not provide fine-grained distinctions between different attack types.
            
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.9\textwidth]{../figures/plots/section3/circular_wordclouds.png}
                \caption{Word Clouds for Each Cluster}
                \label{fig:word_clouds}
            \end{figure}
            
            \subsubsection{Community Detection \\}
            
                Graph-based community detection was applied to further segment clusters into subgroups. The results indicate that specific communities focus on distinct attack behaviors:

                \begin{itemize}
                    \item The K-Means-based community detection identified well-defined subgroups, but many commands appear across multiple communities, indicating overlapping behaviors.
                    \item The GMM-based community detection aligns with K-Means results, with some additional flexibility in capturing transitional attack patterns. However, the persistence of a dominant cluster in both methods suggests that neither approach fully separates attacks into meaningful, non-overlapping categories.
                \end{itemize}
                
                Overall, while community detection adds another layer of granularity, the results indicate that SSH attack behaviors exhibit significant overlap, making strict segmentation challenging.
                
                \begin{figure}[h]
                    \centering
                    \begin{minipage}[c]{0.47\textwidth}
                        \centering
                        \includegraphics[width=0.8\textwidth]{../figures/plots/section3/k-means_graph_visualization_of_cluster_0_with_communities.png}
                        \caption{Community Detection in Cluster 0 (K-Means).}
                        \label{fig:kmeans_graph}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[c]{0.47\textwidth}
                        \centering
                        \includegraphics[width=0.8\textwidth]{../figures/plots/section3/gmm_graph_visualization_of_cluster_0_with_communities.png}
                        \caption{Community Detection in Cluster 0 (GMM).}
                        \label{fig:gmm_graph}
                    \end{minipage}
                \end{figure}
            

    \subsection{Conclusion}
        

        This analysis provided insights into SSH attack patterns through clustering, using both K-Means and GMM. While validation metrics and hyperparameter tuning optimized the models, the visualized results indicate significant overlap between clusters, suggesting challenges in achieving well-separated attack groupings. The presence of dominant clusters encompassing much of the dataset hints at either insufficient feature discrimination or limitations in the chosen clustering techniques. Despite these challenges, the study demonstrates the potential of unsupervised learning in revealing underlying structures in SSH attack data, forming a basis for further refinement in cybersecurity threat detection.
        