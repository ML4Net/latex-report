% 05-unsupervised-learning-clustering.tex

% Unsupervised Learning â€“ Clustering
% 5.1. Introduction: Provides an overview of the clustering task and its objectives.
% 5.2. Determine the Number of Clusters: Uses methods like the elbow method or silhouette analysis to determine the number of clusters.
% 5.3. Hyperparameter Tuning: Tunes other hyperparameters, if any.
% 5.4. Cluster Visualization: Visualizes the clusters through t-SNE.
% 5.5. Cluster Analysis: Analyzes the characteristics of each cluster.
% 5.6. Intent Homogeneity: Assesses if clusters reflect intent division.
% 5.7. Specific Attack Categories: Associates clusters with specific attack categories.

% Section Title
\section{UNSUPERVISED LEARNING - CLUSTERING}

    % Main Content

    \subsection{Introduction}
    
        This section provides an overview of the clustering task and its objectives. The goal is to group similar attack sessions into clusters based on their characteristics. By identifying clusters, we can gain insights into common patterns and behaviors in the attack data. We will use various clustering techniques and evaluate their effectiveness.

    \subsection{Determine the Number of Clusters}
    
        Determining the optimal number of clusters is a crucial step in the clustering process. We use methods like the elbow method and silhouette analysis to identify the appropriate number of clusters.

        \textbf{Elbow Method:} The elbow method involves plotting the sum of squared distances (inertia) for a range of cluster numbers and identifying the point where the inertia starts to decrease more slowly (the "elbow").

        % Elbow Method for k-Means Clustering
        \begin{lstlisting}[caption={Elbow Method for k-Means Clustering}, label={lst:elbow_method}]
            # Elbow Method
            n_cluster_list = []
            inertia_list = []
            for n_clusters in range(3, 17):
                kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)
                kmeans.fit(X)
                inertia_list.append(kmeans.inertia_)
                n_cluster_list.append(n_clusters)
            
            # Plot Elbow Method
            plt.figure(figsize=(5, 3.5))
            plt.plot(n_cluster_list, inertia_list, marker='o', markersize=5, color='blue')
            plt.xlabel('Number of clusters')
            plt.ylabel('k-Means clustering error')
            plt.title('Elbow Method')
            plt.show()
        \end{lstlisting}
        
        \vspace{1em}

        \textbf{Placeholder for Elbow Method Plot}

        \textbf{Silhouette Analysis:} Silhouette analysis involves calculating the silhouette score for each potential cluster number. The silhouette score measures how similar an object is to its own cluster compared to other clusters.

        % Silhouette Analysis for k-Means Clustering
        \begin{lstlisting}[caption={Silhouette Analysis for k-Means Clustering}, label={lst:silhouette_analysis}]
            # Silhouette Analysis
            silhouette_list = []
            for n_clusters in range(3, 17):
                kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)
                labels = kmeans.fit_predict(X)
                silhouette_score_value = silhouette_score(X, labels)
                silhouette_list.append(silhouette_score_value)
            
            # Plot Silhouette Analysis
            plt.figure(figsize=(5, 3.5))
            plt.plot(n_cluster_list, silhouette_list, marker='o', markersize=5, color='blue')
            plt.xlabel('Number of clusters')
            plt.ylabel('Silhouette Score')
            plt.title('Silhouette Analysis')
            plt.show()
        \end{lstlisting}
        
        \vspace{1em}

        \textbf{Placeholder for Silhouette Analysis Plot}

        Based on the elbow method and silhouette analysis, we select the optimal number of clusters for further analysis.
            
    \subsection{Hyperparameter Tuning}
    
        Hyperparameter tuning involves optimizing the parameters of the clustering algorithms to improve their performance. We use GridSearchCV to perform an exhaustive search over specified parameter values.

        \textbf{K-Means Hyperparameter Tuning:}

        % Grid Search for k-Means Clustering
        \begin{lstlisting}[caption={Grid Search for k-Means Clustering}, label={lst:grid_search_kmeans}]
            # Define parameter grid for K-Means
            param_grid_kmeans = {
                'init': ['k-means++', 'random'],
                'n_init': list(range(10, 21, 2)),
                'max_iter': list(range(50, 200, 50)),
            }
            
            # Create KMeans object
            kmeans = KMeans(n_clusters=10, random_state=42)
            
            # Create GridSearchCV object
            grid_search_kmeans = GridSearchCV(kmeans, param_grid=param_grid_kmeans, cv=5)
            
            # Fit the grid search to the data
            grid_search_kmeans.fit(X)
            
            # Get the best parameters
            best_params_kmeans = grid_search_kmeans.best_params_
            print("Best parameters:", best_params_kmeans)
        \end{lstlisting}
        
        \vspace{1em}

        \textbf{Placeholder for K-Means Hyperparameter Tuning Results}

        \textbf{Gaussian Mixture Model (GMM) Hyperparameter Tuning:}

        % Grid Search for Gaussian Mixture Model (GMM)
        \begin{lstlisting}[caption={Grid Search for Gaussian Mixture Model (GMM)}, label={lst:grid_search_gmm}]
            # Define parameter grid for GMM
            param_grid_gmm = {
                'init_params': ['kmeans'],
                'covariance_type': ['full', 'spherical'],
                'tol': [1e-3, 1e-4, 1e-5],
                'max_iter': list(range(50, 300, 50)),
            }
            
            # Create GaussianMixture object
            gmm = GaussianMixture(n_components=10, random_state=42)
            
            # Create GridSearchCV object
            grid_search_gmm = GridSearchCV(gmm, param_grid=param_grid_gmm, cv=5, scoring=silhouette_scorer)
            
            # Fit the grid search to the data
            grid_search_gmm.fit(X)
            
            # Get the best parameters
            best_params_gmm = grid_search_gmm.best_params_
            print("Best parameters:", best_params_gmm)
        \end{lstlisting}
        
        \vspace{1em}

        \textbf{Placeholder for GMM Hyperparameter Tuning Results}
            
    \subsection{Cluster Visualization}
    
        Visualizing the clusters helps in understanding the distribution and characteristics of the clusters. We use t-SNE to reduce the dimensionality of the data and create clear visual representations of the clusters.

        \textbf{t-SNE Visualization:}

        % t-SNE Visualization of Clusters
        \begin{lstlisting}[caption={t-SNE Visualization of Clusters}, label={lst:tsne_visualization}]
            # Apply t-SNE to reduce the number of components
            tsne = TSNE(n_components=2, random_state=42).fit_transform(X)
            df_tsne = pd.DataFrame(tsne, columns=['x1', 'x2'])
            
            # K-Means Clusters
            df_tsne['cluster_kmeans'] = kmeans_tuned.labels_
            sns.scatterplot(data=df_tsne, x='x1', y='x2', hue='cluster_kmeans', palette='viridis')
            plt.title('t-SNE Visualization of K-Means Clusters')
            plt.show()
            
            # GMM Clusters
            df_tsne['cluster_gmm'] = gmm_tuned.predict(X)
            sns.scatterplot(data=df_tsne, x='x1', y='x2', hue='cluster_gmm', palette='viridis')
            plt.title('t-SNE Visualization of GMM Clusters')
            plt.show()
        \end{lstlisting}
        
        \vspace{1em}

        \textbf{Placeholder for t-SNE Visualization of K-Means Clusters}

        \textbf{Placeholder for t-SNE Visualization of GMM Clusters}
            
    \subsection{Cluster Analysis}
    
        Analyzing the characteristics of each cluster helps in understanding the common patterns and behaviors within the clusters. We examine the distribution of features and intents within each cluster.

        \textbf{Cluster Feature Analysis:}

        % Feature Distribution Analysis by Cluster
        \begin{lstlisting}[caption={Feature Distribution Analysis by Cluster}, label={lst:feature_distribution}]
            # Analyze the distribution of features within each cluster
            for cluster in range(10):
                cluster_data = df_tsne[df_tsne['cluster_kmeans'] == cluster]
                print(f"Cluster {cluster} Feature Distribution:")
                print(cluster_data.describe())
        \end{lstlisting}
        
        \vspace{1em}

        \textbf{Placeholder for Cluster Feature Distribution Tables}
            
    \subsection{Intent Homogeneity}
    
        Assessing if clusters reflect intent division involves examining the homogeneity of intents within each cluster. We calculate the proportion of each intent within the clusters to determine if the clusters are homogeneous.

        \textbf{Intent Homogeneity Analysis:}

        % Intent Proportions Analysis by Cluster
        \begin{lstlisting}[caption={Intent Proportions Analysis by Cluster}, label={lst:intent_proportions}]
            # Calculate the proportion of each intent within the clusters
            for cluster in range(10):
                cluster_data = df_tsne[df_tsne['cluster_kmeans'] == cluster]
                intent_proportions = cluster_data['intent'].value_counts(normalize=True)
                print(f"Cluster {cluster} Intent Proportions:")
                print(intent_proportions)
        \end{lstlisting}
        
        \vspace{1em}

        \textbf{Placeholder for Intent Proportion Tables}
        
    \subsection{Specific Attack Categories}
    
        Associating clusters with specific attack categories involves identifying the common attack patterns within each cluster. We analyze the most frequent attack categories within the clusters to understand their characteristics.

        \textbf{Attack Category Analysis:}

        % Attack Categories Analysis by Cluster
        \begin{lstlisting}[caption={Attack Categories Analysis by Cluster}, label={lst:attack_categories}]
            # Analyze the most frequent attack categories within the clusters
            for cluster in range(10):
                cluster_data = df_tsne[df_tsne['cluster_kmeans'] == cluster]
                attack_categories = cluster_data['attack_category'].value_counts()
                print(f"Cluster {cluster} Attack Categories:")
                print(attack_categories)
        \end{lstlisting}
        
        \vspace{1em}

        \textbf{Placeholder for Attack Category Distribution Tables}
